# ğŸ™ï¸ Text-to-Podcast Automation (Text2Podcast)

**[Hybrid Cloud/Local System]** An automated pipeline that scrapes online forums like Reddit, cleans text using AI, synthesizes a podcast-style audio file via Azure TTS, and publishes it to a private RSS feed and Telegram channel.

---

## ğŸ—ï¸ System Architecture

The project is built to handle the transition from a local development environment to a restricted Azure Functions (Linux) environment.

### **The "Future Me" Cheat Sheet**

* **Audio Engine:** Uses `pydub` + `Azure Speech SDK`.
* **The FFmpeg Workaround:** Azure Functions are read-only. We bundle `amd64` static binaries in `/bin`. The code automatically copies them to `/tmp` and sets executable permissions on every run.
* **Memory:** Uses **Azure Table Storage** (`PostHistory`) to ensure we never process the same Reddit post twice.
* **Hosting:** Audio files and the `feed.xml` live in **Azure Blob Storage**.

---

## ğŸ“ File Directory & Roles

### **Core Orchestration**

* **`function_app.py` [Cloud Only]:** The entry point. Contains the Timer Trigger (hourly) and the HTTP Trigger (manual URL).
* **`main.py` [Hybrid]:** The "Brain" that coordinates the modules.
* **`config.py` [Hybrid]:** Centralizes keys. **Crucial:** Detects `IS_CLOUD` to adjust file paths automatically.

### **Heavy Lifter Modules (`/modules`)**

* **`llm_cleaner.py`:** Uses DeepSeek to turn Reddit HTML into a clean script. Includes a "JSON Rescue" regex for cut-off AI responses.
* **`tts_engine.py`:** Handles the "Nuclear" FFmpeg setup and synthesizes the voice.
* **`rss_generator.py`:** Updates the XML feed so the podcast appears in your app.
* **`telegram_bot.py`:** Sends the final MP3 to your phone. **Note:** `parse_mode` is disabled to prevent crashes from weird characters in titles.

### **Maintenance & Dev Tools**

* **`test_single_post.py` [Local Only]:** Use this to test a specific PostID on your laptop without touching the cloud.
* **`manual_trigger.py` [Local Only]:** Force-processes specific IDs into the production feed.
* **`sync_feed_logics.py` [Local Only]:** Use this if the RSS feed and Blob storage get out of sync.

---

## ğŸ“¦ Infrastructure Setup

Before deploying, you must configure the following in the Azure Portal:

### **1. Azure Blob Storage (The Hosting House)**
* **Container Name:** `podcasts` (Must match `AZURE_STORAGE_CONTAINER` in `config.py`).
* **Access Level:** Set to **"Blob" (anonymous read access for blobs only)**. This allows your podcast app to download the MP3s and `feed.xml` without a key.
* **Structure:** The system will automatically upload episodes, scripts, and the RSS feed here.

### **2. Azure Table Storage (The Bookkeeper)**
* **Table Name:** `PostHistory` (Must match `AZURE_TABLE_NAME` in `config.py`).
* **Role:** Acts as a "deduplication" engine to ensure you don't get the same podcast twice.

### **3. Azure Function App (The Engine)**
* **Runtime Stack:** Python 3.12 (Linux).
* **Hosting Plan:** Consumption (Serverless) is recommended for low costs.
* **Environment Variables (Application Settings):** Because we refactored `config.py` to be secure, you **must** manually add your keys to the Azure Portal under **Settings > Environment variables**. Add the following:
    - `AZURE_SPEECH_KEY`
    - `AZURE_SPEECH_REGION`
    - `AZURE_STORAGE_CONNECTION_STRING`
    - `DEEPSEEK_API_KEY`
    - `TELEGRAM_TOKEN`
    - `TELEGRAM_CHAT_ID`
* **Note:** Without these variables, the cloud version will crash immediately even if your code is perfect.

---

## ğŸš€ Deployment to Azure

### **1. Local Setup**
1. Ensure you have a **Python 3.12+** environment.
2. Install Mac dependencies: `brew install ffmpeg`.
3. Install Python libraries: `pip install -r requirements.txt`.
4. Ensure `local.settings.json` matches your `config.py` keys.

---

### **2. Deploying to Azure**
Use the Azure Functions Core Tools to push your code to the cloud:
```bash
func azure functionapp publish text2podcast-service-01

```

---

### **3. Validate with Manual Trigger **

Once deployed, the `timer_trigger` will run hourly. However, you should **manually trigger** the function immediately after deployment to validate that the cloud environment (FFmpeg paths, Storage keys, etc.) is working correctly.

**A. Fetch your Secret Trigger URL:**
To run the function on demand, you need the unique API key generated by Azure:

```bash
func azure functionapp function show --name manual_run --resource-group [Your-RG] --app-name text2podcast-service-01 --show-keys

```

Copy the `invokeUrlTemplate` (it will look like `https://.../api/manual_run?code=...`).

**B. Execute the Trigger:**
Simply paste that URL into your browser or use `curl`:

```bash
curl "[https://your-app.azurewebsites.net/api/manual_run?code=YOUR_SECRET_KEY](https://your-app.azurewebsites.net/api/manual_run?code=YOUR_SECRET_KEY)"

```

---

### **4. Monitoring & Debugging **

Triggering the function is only half the battle; you need to see the "Log Stream" to catch errors like `Exec format error` or `API Key Unauthorized`.

1. Go to your **Function App** > **Functions** > **manual_run**.
2. Click **Invocations** to see a history of past runs.
3. Click **Logs** at the bottom of the screen to see the real-time console output.

**What to look for in the logs:**

* `ğŸ™ï¸ Using Gender: ...`: Confirms LLM cleaning and inference is working.
* `âœ… Uploaded to Blob: ...`: Confirms your storage connection string is correct.
* `ğŸ”¥ CRITICAL CRASH`: If you see this, check the tracebackâ€”it usually points to a missing Environment Variable in the Azure Portal.
